#parsing
import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
ua = UserAgent()

# aiogram
from aiogram.dispatcher.filters import Command, Text
from aiogram.types import Message, CallbackQuery, ParseMode

# built-in
from time import sleep

# modules
from config import dp, bot

# translate (googletrans==3.1.0a0)
from googletrans import Translator

parsing = True

@dp.message_handler(Command("start"))
async def start_command(message: Message):
	await message.answer("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å —Å –∫–æ–º–∞–Ω–¥–æ–π /q. –ü—Ä–∏–º–µ—Ä:\n\n/q diabetes\n\n–ß—Ç–æ–±—ã –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å—ã, –≤–≤–µ–¥–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É /stop")


@dp.message_handler(Command("q"))
async def parsing_query(message: Message):
	global parsing
	parsing = True

	pagination_count = 0

	article_count = 1

	for pagination_count in range(1, 6):
		headers = { "User-Agent": ua.random, } # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –≤ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö

		request = message.text.replace("/q ", "") # –£–±—Ä–∞—Ç—å –∫–æ–º–∞–Ω–¥—É –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞

		# –°—Å—ã–ª–∫–∞ –Ω–∞ —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –¥–æ—Å—Ç—É–ø–µ–Ω abstract
		url = f"https://pubmed.ncbi.nlm.nih.gov/?term={request}&filter=simsearch1.fha&page={pagination_count}"

		main_url = "https://pubmed.ncbi.nlm.nih.gov" # –û—Å–Ω–æ–≤–Ω–æ–π –∞–¥—Ä–µ—Å —Å–∞–π—Ç–∞


		response = requests.get(url, headers = headers) # –ó–∞–ø—Ä–æ—Å –∫ —Å—Ç—Ä–∞–Ω–∏—Ü–µ

		soup = BeautifulSoup(response.text, "lxml") # –ü–∞—Ä—Å–∏–Ω–≥ html-–∫–æ–¥–∞, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ DOM-—ç–ª–µ–º–µ–Ω—Ç—ã

		data_links = soup.find_all("a", class_="docsum-title") # –°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏ –≤ —Å–ª–æ–≤–∞—Ä—å

		translator = Translator()

		# –ü–µ—Ä–µ—Ö–æ–¥ –ø–æ —Å—Å—ã–ª–∫–∞–º
		for link in data_links:
			sleep(3) # –ü–µ—Ä–µ—Ä—ã–≤ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ 3 —Å–µ–∫

			link = link.get("href") # –ü–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–∞ 'href' —É –∫–∞–∂–¥–æ–≥–æ —Ç—ç–≥–∞ 'a'
			link = main_url + link.strip() # –î–æ–±–∞–≤–∏—Ç—å –ø–µ—Ä–µ–¥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π —Å—Å—ã–ª–∫–æ–π –æ—Å–Ω–æ–≤–Ω–æ–π –∞–¥—Ä–µ—Å —Å–∞–π—Ç–∞, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏–ª–∞—Å—å –ø–æ–ª–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç–∞—Ç—å—é

			article_response = requests.get(link, headers = headers) # –ó–∞–ø—Ä–æ—Å –∫ –∫–∞–∂–¥–æ–π –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–æ —Å—Ç–∞—Ç—å—è–º–∏

			article_soup = BeautifulSoup(article_response.text, "lxml") # –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–π –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–æ —Å—Ç–∞—Ç—å—è–º–∏

			article_title = article_soup.find("h1", class_="heading-title") # –ù–∞–π—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–µ
			article_title = article_title.text # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏
			article_title_translated = translator.translate(article_title, src = "en", dest = "ru") # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏
			article_title_translated = article_title_translated.text

			article_abstract = ""
			article_abstract_translated = ""

			# –ù–∞–π—Ç–∏ –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ –±–ª–æ–∫–µ Abstract
			article_abstract_paragraphs = article_soup.find("div", class_="abstract-content").find_all("p")

			# –î–æ–±–∞–≤–∏—Ç—å –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
			for article_p in article_abstract_paragraphs:
				if parsing:
					article_abstract += article_p.text + "\n"

					# –ü–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–π Abstract
					article_abstract_translated += translator.translate(article_p.text, src = "en", dest = "ru").text

					# –û—Ç–≤–µ—Ç –±–æ—Ç–∞
					# –ï—Å–ª–∏ —É –±–æ—Ç–∞ –µ—Å—Ç—å –∑–∞–∫–æ–Ω—á–∏–ª—Å—è –ª–∏–º–∏—Ç –Ω–∞ –ø–µ—Ä–µ–≤–æ–¥—ã, —Ç–æ –≤–µ—Ä–Ω—É—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
					if len(article_abstract.strip()) < 4096 and len(article_abstract_translated) < 4096:
						await message.answer(
							# –ù–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏ (—Å –∫–æ–Ω—Ü–∞)
							str(article_count) + 

							# –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–æ—Ä–∏–≥–∏–Ω–∞–ª)
							"\n\n\n" + "* Title:" + 
							"\n\n" + article_title.strip() +
				
							# Abstract
							"\n\n\n" + "* Abstract:" +
							"\n\n" + article_abstract.strip(),
				
							# parse_mode = "HTML"
						)

						sleep(2) # –ü–µ—Ä–µ—Ä—ã–≤ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ 2 —Å–µ–∫

						await message.answer(
							f"üëÜ–ü–µ—Ä–µ–≤–æ–¥ —Å—Ç–∞—Ç—å–∏ {article_count}:"

							# –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏
							"\n\n\n" + "* –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–ü–µ—Ä–µ–≤–æ–¥):" + 
							"\n\n" + article_title_translated.strip() + 

							# –ü–µ—Ä–µ–≤–æ–¥ abstract
							"\n\n\n" + "* –ê–±—Å—Ç—Ä–∞–∫—Ç (–ü–µ—Ä–µ–≤–æ–¥):" 
							"\n\n" + article_abstract_translated
						)
					else:
						await message.answer(
							# –ù–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏ (—Å –∫–æ–Ω—Ü–∞)
							str(article_count) + 

							# –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–æ—Ä–∏–≥–∏–Ω–∞–ª)
							"\n\n\n" + "* –ó–∞–≥–æ–ª–æ–≤–æ–∫:" + 
							"\n\n" + article_title.strip() +

							# –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏
							"\n\n\n" + "* –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–ü–µ—Ä–µ–≤–æ–¥):" + 
							"\n\n" + article_title_translated.strip() + 
					
							# Abstract
							"\n\n\n" + "* Abstract:" +
							"\n\n" + "Abstract –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ —Å–æ–æ–±—â–µ–Ω–∏–µ Telegram (—Å–∏–º–≤–æ–ª–æ–≤ –±–æ–ª—å—à–µ 4096).",
							"\n\n" + f"–°—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç–∞—Ç—å—é: {link}"
					
							# parse_mode = "HTML"
						)
				else:
					break

				article_count += 1 # –°—á—ë—Ç—á–∏–∫ —Å—Ç–∞—Ç–µ–π (—Å –∫–æ–Ω—Ü–∞)

				if not parsing:
					break
			
			if not parsing:
				break

		if not parsing:
			break



@dp.message_handler(Command("stop"))
async def stop(message: Message):
	global parsing
	parsing = False
	await message.answer("–ó–∞–ø—Ä–æ—Å—ã –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã.")
	await message.answer("–ó–∞–ø—Ä–æ—Å—ã –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã.")